{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "website_data = pd.read_table('data/links.txt', header=0, dtype='str', encoding='utf-8', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#take sample of 50 websites for testing (smaller matrix)\n",
    "sample50 = website_data.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_soup(url):\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        html = r.content\n",
    "        data = BeautifulSoup(html, 'lxml')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching:  81\n",
      "searching:  137\n",
      "searching:  10\n",
      "searching:  60\n",
      "searching:  18\n",
      "searching:  133\n",
      "searching:  44\n",
      "searching:  78\n",
      "searching:  92\n",
      "searching:  145\n",
      "searching:  195\n",
      "searching:  79\n",
      "searching:  4\n",
      "searching:  45\n",
      "searching:  127\n",
      "searching:  50\n",
      "searching:  53\n",
      "searching:  192\n",
      "searching:  46\n",
      "searching:  40\n",
      "searching:  19\n",
      "searching:  51\n",
      "searching:  129\n",
      "searching:  167\n",
      "searching:  31\n",
      "searching:  138\n",
      "searching:  74\n",
      "searching:  39\n",
      "searching:  178\n",
      "searching:  180\n",
      "searching:  95\n",
      "searching:  111\n",
      "searching:  114\n",
      "searching:  85\n",
      "searching:  136\n",
      "searching:  142\n",
      "searching:  156\n",
      "searching:  26\n",
      "searching:  130\n",
      "searching:  9\n",
      "searching:  194\n",
      "searching:  196\n",
      "searching:  175\n",
      "searching:  1\n",
      "searching:  56\n",
      "searching:  109\n",
      "searching:  140\n",
      "searching:  82\n",
      "searching:  184\n",
      "searching:  70\n",
      "FINISHED\n"
     ]
    }
   ],
   "source": [
    "#get link bodies\n",
    "websites = {}\n",
    "\n",
    "for i,d in sample50.iterrows():\n",
    "    \n",
    "    print('searching: ', i)\n",
    "    soup = make_soup(d['link'])\n",
    "    \n",
    "    if soup != None:\n",
    "        \n",
    "        #remove css <style> and javascript <script> elements from soup\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "        \n",
    "        #get text from soup\n",
    "        text = soup.get_text()\n",
    "        #split lines, strip and remove nulls (blank lines)\n",
    "        lines = [line.strip() for line in text.splitlines() if line]\n",
    "        #join it back\n",
    "        text = '\\n'.join(lines)\n",
    "        \n",
    "        #add link to websites\n",
    "        websites[d['link']] = {}\n",
    "        #add link title to link object\n",
    "        websites[d['link']]['title'] = (soup.title.text if soup.title else '')\n",
    "        #add link text to link object\n",
    "        websites[d['link']]['text'] = text\n",
    "        \n",
    "print('FINISHED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save all text documents as list\n",
    "all_text = [link['text'] for link in websites.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "train_matrix_cnt = count_vect.fit_transform(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse matrix shape: (50, 5885)\n",
      "size: 294250\n",
      "non-zeros: 14711\n",
      "sparsity: 95.00%\n",
      "density: 5.00%\n"
     ]
    }
   ],
   "source": [
    "#explore sparse matrix\n",
    "print('sparse matrix shape:', train_matrix_cnt.shape)\n",
    "print('size:', (train_matrix_cnt.shape[0] * train_matrix_cnt.shape[1]))\n",
    "print('non-zeros:', train_matrix_cnt.getnnz())\n",
    "print('sparsity: %.2f%%' % (100.0 * (((train_matrix_cnt.shape[0] * train_matrix_cnt.shape[1]) - train_matrix_cnt.getnnz()) / (train_matrix_cnt.shape[0] * train_matrix_cnt.shape[1]))))\n",
    "print('density: %.2f%%' % (100.0 * train_matrix_cnt.getnnz() / (train_matrix_cnt.shape[0] * train_matrix_cnt.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Fit and Transform count sparse matrix to normalized tf-idf sparse matrix\n",
    "train_matrix_tfidf = TfidfTransformer().fit_transform(train_matrix_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% complete\n",
      "2% complete\n",
      "4% complete\n",
      "6% complete\n",
      "8% complete\n",
      "10% complete\n",
      "12% complete\n",
      "14% complete\n",
      "16% complete\n",
      "18% complete\n",
      "20% complete\n",
      "22% complete\n",
      "24% complete\n",
      "26% complete\n",
      "28% complete\n",
      "30% complete\n",
      "32% complete\n",
      "34% complete\n",
      "36% complete\n",
      "38% complete\n",
      "40% complete\n",
      "42% complete\n",
      "44% complete\n",
      "46% complete\n",
      "48% complete\n",
      "50% complete\n",
      "52% complete\n",
      "54% complete\n",
      "56% complete\n",
      "58% complete\n",
      "60% complete\n",
      "62% complete\n",
      "64% complete\n",
      "66% complete\n",
      "68% complete\n",
      "70% complete\n",
      "72% complete\n",
      "74% complete\n",
      "76% complete\n",
      "78% complete\n",
      "80% complete\n",
      "82% complete\n",
      "84% complete\n",
      "86% complete\n",
      "88% complete\n",
      "90% complete\n",
      "92% complete\n",
      "94% complete\n",
      "96% complete\n",
      "98% complete\n",
      "100%% complete\n"
     ]
    }
   ],
   "source": [
    "#construct square doc matrix, of cosine-similarity values, using pandas dataframe\n",
    "X = pd.DataFrame()\n",
    "\n",
    "for i in range(len(websites)):\n",
    "    \n",
    "    complete = round((i/len(websites))*100)\n",
    "    if complete % 2 == 0:\n",
    "        print('%s%% complete' % complete)\n",
    "    \n",
    "    #compute dot product cosine similarity of indexed document with all others\n",
    "    cosine_similarities = linear_kernel(train_matrix_tfidf[i], train_matrix_tfidf).flatten()\n",
    "    s = pd.Series(cosine_similarities)\n",
    "    df = pd.DataFrame(s, index=list(s.index)).T\n",
    "    X = X.append(df, ignore_index=True)\n",
    "\n",
    "print('100%% complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#construct document similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write matrix cols\n",
    "with open('website_matrix/d3matrix_datacols.txt', 'w') as f:\n",
    "    for link in websites.keys():\n",
    "        f.write(link + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "header = ['link1', 'link2', 'value']\n",
    "h = pd.DataFrame(columns=header)\n",
    "h.to_csv('website_matrix/d3matrix_data.tsv', mode='w', sep='\\t', encoding='utf-8', index=False, header=True)\n",
    "\n",
    "#l1, l2 are viz matrix indices\n",
    "#index1, index2 are sparse matrix indices\n",
    "\n",
    "keys = list(websites.keys())\n",
    "\n",
    "l1 = 1\n",
    "for link1 in websites.keys():\n",
    "    index1 = keys.index(link1)\n",
    "    l2 = 1\n",
    "    for link2 in websites.keys():\n",
    "        index2 = keys.index(link2)\n",
    "        co = (X.iloc[index1][index2] if link1 != link2 else 0)\n",
    "        r = [l1, l2, co]\n",
    "        row = pd.Series(r, index=header)\n",
    "        row = pd.DataFrame(r).T\n",
    "        row.to_csv('website_matrix/d3matrix_data.tsv', mode='a', sep='\\t', encoding='utf-8', index=False, header=False)\n",
    "        l2+=1\n",
    "    l1+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
