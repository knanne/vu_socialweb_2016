{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "website_data = pd.read_table('data/links_annotated.txt', header=0, dtype='str', encoding='utf-8', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "website_data = website_data[website_data['title'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KIN</td>\n",
       "      <td>Marleen Huysman</td>\n",
       "      <td>http://www.kinresearch.nl/person/marleen-huysman/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KIN</td>\n",
       "      <td>Bart van den Hooff</td>\n",
       "      <td>http://www.kinresearch.nl/person/bart-van-den-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KIN</td>\n",
       "      <td>Han Gerrits</td>\n",
       "      <td>http://www.kinresearch.nl/person/han-gerrits/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KIN</td>\n",
       "      <td>Raghu Garud</td>\n",
       "      <td>http://www.kinresearch.nl/person/raghu-garud/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KIN</td>\n",
       "      <td>Samer Faraj</td>\n",
       "      <td>http://www.kinresearch.nl/samer-faraj/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  group               title                                               link\n",
       "0   KIN     Marleen Huysman  http://www.kinresearch.nl/person/marleen-huysman/\n",
       "1   KIN  Bart van den Hooff  http://www.kinresearch.nl/person/bart-van-den-...\n",
       "2   KIN         Han Gerrits      http://www.kinresearch.nl/person/han-gerrits/\n",
       "3   KIN         Raghu Garud      http://www.kinresearch.nl/person/raghu-garud/\n",
       "4   KIN         Samer Faraj             http://www.kinresearch.nl/samer-faraj/"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_soup(url):\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        html = r.content\n",
    "        data = BeautifulSoup(html, 'lxml')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching:  0\n",
      "searching:  1\n",
      "searching:  2\n",
      "searching:  3\n",
      "searching:  4\n",
      "searching:  5\n",
      "searching:  6\n",
      "searching:  7\n",
      "searching:  8\n",
      "searching:  9\n",
      "searching:  10\n",
      "searching:  11\n",
      "searching:  12\n",
      "searching:  13\n",
      "searching:  14\n",
      "searching:  15\n",
      "searching:  16\n",
      "searching:  17\n",
      "searching:  18\n",
      "searching:  19\n",
      "searching:  20\n",
      "searching:  21\n",
      "searching:  22\n",
      "searching:  23\n",
      "searching:  24\n",
      "searching:  25\n",
      "searching:  26\n",
      "searching:  27\n",
      "searching:  28\n",
      "searching:  29\n",
      "searching:  30\n",
      "searching:  31\n",
      "searching:  32\n",
      "searching:  33\n",
      "searching:  34\n",
      "searching:  35\n",
      "searching:  36\n",
      "searching:  37\n",
      "searching:  38\n",
      "searching:  39\n",
      "searching:  40\n",
      "searching:  41\n",
      "searching:  42\n",
      "searching:  43\n",
      "searching:  44\n",
      "searching:  45\n",
      "searching:  46\n",
      "searching:  47\n",
      "searching:  48\n",
      "searching:  49\n",
      "searching:  50\n",
      "searching:  51\n",
      "searching:  52\n",
      "searching:  53\n",
      "searching:  54\n",
      "searching:  55\n",
      "searching:  56\n",
      "searching:  57\n",
      "searching:  58\n",
      "searching:  59\n",
      "searching:  60\n",
      "searching:  61\n",
      "searching:  62\n",
      "searching:  63\n",
      "searching:  64\n",
      "searching:  65\n",
      "searching:  67\n",
      "searching:  68\n",
      "searching:  69\n",
      "searching:  70\n",
      "searching:  71\n",
      "searching:  72\n",
      "searching:  73\n",
      "searching:  74\n",
      "searching:  75\n",
      "searching:  76\n",
      "searching:  77\n",
      "searching:  78\n",
      "searching:  79\n",
      "searching:  80\n",
      "searching:  81\n",
      "searching:  82\n",
      "searching:  83\n",
      "searching:  84\n",
      "searching:  85\n",
      "searching:  86\n",
      "searching:  87\n",
      "searching:  88\n",
      "searching:  89\n",
      "searching:  90\n",
      "searching:  91\n",
      "searching:  92\n",
      "searching:  93\n",
      "searching:  94\n",
      "searching:  95\n",
      "searching:  96\n",
      "searching:  97\n",
      "searching:  98\n",
      "searching:  99\n",
      "searching:  100\n",
      "searching:  101\n",
      "searching:  102\n",
      "searching:  103\n",
      "searching:  104\n",
      "searching:  105\n",
      "searching:  106\n",
      "searching:  107\n",
      "searching:  108\n",
      "searching:  109\n",
      "searching:  110\n",
      "searching:  111\n",
      "searching:  112\n",
      "searching:  113\n",
      "searching:  114\n",
      "searching:  115\n",
      "searching:  116\n",
      "searching:  117\n",
      "searching:  118\n",
      "searching:  119\n",
      "searching:  120\n",
      "searching:  121\n",
      "searching:  122\n",
      "searching:  123\n",
      "searching:  124\n",
      "searching:  125\n",
      "searching:  126\n",
      "searching:  127\n",
      "searching:  128\n",
      "searching:  129\n",
      "searching:  130\n",
      "searching:  131\n",
      "searching:  132\n",
      "searching:  133\n",
      "searching:  134\n",
      "searching:  135\n",
      "searching:  136\n",
      "searching:  137\n",
      "searching:  138\n",
      "searching:  139\n",
      "searching:  140\n",
      "searching:  141\n",
      "searching:  142\n",
      "searching:  143\n",
      "searching:  144\n",
      "searching:  145\n",
      "searching:  146\n",
      "searching:  147\n",
      "searching:  148\n",
      "searching:  149\n",
      "searching:  150\n",
      "searching:  151\n",
      "searching:  152\n",
      "searching:  153\n",
      "searching:  154\n",
      "searching:  155\n",
      "searching:  156\n",
      "searching:  157\n",
      "searching:  158\n",
      "searching:  159\n",
      "searching:  160\n",
      "searching:  161\n",
      "searching:  162\n",
      "searching:  163\n",
      "searching:  164\n",
      "searching:  165\n",
      "searching:  166\n",
      "searching:  167\n",
      "searching:  168\n",
      "searching:  169\n",
      "searching:  170\n",
      "searching:  171\n",
      "searching:  172\n",
      "searching:  173\n",
      "searching:  174\n",
      "searching:  175\n",
      "searching:  176\n",
      "searching:  177\n",
      "searching:  178\n",
      "searching:  179\n",
      "searching:  180\n",
      "searching:  181\n",
      "searching:  182\n",
      "searching:  183\n",
      "searching:  184\n",
      "searching:  185\n",
      "searching:  186\n",
      "searching:  187\n",
      "searching:  189\n",
      "searching:  192\n",
      "searching:  193\n",
      "searching:  194\n",
      "searching:  195\n",
      "searching:  196\n",
      "FINISHED\n"
     ]
    }
   ],
   "source": [
    "#get link bodies\n",
    "websites = {}\n",
    "\n",
    "for i,d in website_data.iterrows():\n",
    "    \n",
    "    print('searching: ', i)\n",
    "    soup = make_soup(d['link'])\n",
    "    \n",
    "    if soup != None:\n",
    "        \n",
    "        #remove css <style> and javascript <script> elements from soup\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "        \n",
    "        #get text from soup\n",
    "        text = soup.get_text()\n",
    "        #split lines, strip and remove nulls (blank lines)\n",
    "        lines = [line.strip() for line in text.splitlines() if line]\n",
    "        #join it back\n",
    "        text = '\\n'.join(lines)\n",
    "        \n",
    "        #add link name to websites\n",
    "        websites[d['title']] = {}\n",
    "        #add link to object\n",
    "        websites[d['title']]['link'] = d['link']\n",
    "        #add link group to object\n",
    "        websites[d['title']]['group'] = d['group']\n",
    "        #add link text to link object\n",
    "        websites[d['title']]['text'] = text\n",
    "        \n",
    "print('FINISHED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json.dump(websites, open('data/website_text.json','w'), default=str, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/website_text.json') as f:\n",
    "    websites = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#add length of website text\n",
    "for site in websites:\n",
    "    websites[site]['length'] = len(websites[site]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#filter\n",
    "websites_filtered = {}\n",
    "\n",
    "for site in websites:\n",
    "    if websites[site]['length'] > 4000:\n",
    "        websites_filtered.update({site:websites[site]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save all text documents as list\n",
    "all_text = [link['text'] for link in websites_filtered.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "train_matrix_cnt = count_vect.fit_transform(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse matrix shape: (95, 13662)\n",
      "size: 1297890\n",
      "non-zeros: 45264\n",
      "sparsity: 96.51%\n",
      "density: 3.49%\n"
     ]
    }
   ],
   "source": [
    "#explore sparse matrix\n",
    "print('sparse matrix shape:', train_matrix_cnt.shape)\n",
    "print('size:', (train_matrix_cnt.shape[0] * train_matrix_cnt.shape[1]))\n",
    "print('non-zeros:', train_matrix_cnt.getnnz())\n",
    "print('sparsity: %.2f%%' % (100.0 * (((train_matrix_cnt.shape[0] * train_matrix_cnt.shape[1]) - train_matrix_cnt.getnnz()) / (train_matrix_cnt.shape[0] * train_matrix_cnt.shape[1]))))\n",
    "print('density: %.2f%%' % (100.0 * train_matrix_cnt.getnnz() / (train_matrix_cnt.shape[0] * train_matrix_cnt.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Fit and Transform count sparse matrix to normalized tf-idf sparse matrix\n",
    "train_matrix_tfidf = TfidfTransformer().fit_transform(train_matrix_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% complete\n",
      "2% complete\n",
      "4% complete\n",
      "6% complete\n",
      "8% complete\n",
      "12% complete\n",
      "14% complete\n",
      "16% complete\n",
      "18% complete\n",
      "20% complete\n",
      "22% complete\n",
      "24% complete\n",
      "26% complete\n",
      "28% complete\n",
      "32% complete\n",
      "34% complete\n",
      "36% complete\n",
      "38% complete\n",
      "40% complete\n",
      "42% complete\n",
      "44% complete\n",
      "46% complete\n",
      "48% complete\n",
      "52% complete\n",
      "54% complete\n",
      "56% complete\n",
      "58% complete\n",
      "60% complete\n",
      "62% complete\n",
      "64% complete\n",
      "66% complete\n",
      "68% complete\n",
      "72% complete\n",
      "74% complete\n",
      "76% complete\n",
      "78% complete\n",
      "80% complete\n",
      "82% complete\n",
      "84% complete\n",
      "86% complete\n",
      "88% complete\n",
      "92% complete\n",
      "94% complete\n",
      "96% complete\n",
      "98% complete\n",
      "100%% complete\n"
     ]
    }
   ],
   "source": [
    "#construct square doc matrix, of cosine-similarity values, using pandas dataframe\n",
    "X = pd.DataFrame()\n",
    "\n",
    "for i in range(len(websites_filtered)):\n",
    "    \n",
    "    complete = round((i/len(websites_filtered))*100)\n",
    "    if complete % 2 == 0:\n",
    "        print('%s%% complete' % complete)\n",
    "    \n",
    "    #compute dot product cosine similarity of indexed document with all others\n",
    "    cosine_similarities = linear_kernel(train_matrix_tfidf[i], train_matrix_tfidf).flatten()\n",
    "    s = pd.Series(cosine_similarities)\n",
    "    df = pd.DataFrame(s, index=list(s.index)).T\n",
    "    X = X.append(df, ignore_index=True)\n",
    "\n",
    "print('100%% complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#write matrix cols\n",
    "with open('d3matrix_datacols.txt', 'w') as f:\n",
    "    for i,doc in website_data.iterrows():\n",
    "        if doc['title'] in websites_filtered.keys():\n",
    "            f.write(doc['title'] + '\\t' + doc['group'] + '\\t' + doc['link'] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "header = ['link1', 'link2', 'value']\n",
    "h = pd.DataFrame(columns=header)\n",
    "h.to_csv('d3matrix_data.tsv', mode='w', sep='\\t', encoding='utf-8', index=False, header=True)\n",
    "\n",
    "#l1, l2 are viz matrix indices\n",
    "#index1, index2 are matrix indices\n",
    "\n",
    "keys = list(websites_filtered.keys())\n",
    "orderedtitles = list(website_data['title'])\n",
    "\n",
    "t1 = 1\n",
    "for title1 in orderedtitles:\n",
    "    if title1 in websites_filtered.keys():\n",
    "        index1 = keys.index(title1)\n",
    "        t2 = 1\n",
    "        for title2 in orderedtitles:\n",
    "            if title2 in websites_filtered.keys():\n",
    "                index2 = keys.index(title2)\n",
    "                co = (round(X.iloc[index1][index2], 2) if title1 != title2 else 0)\n",
    "                r = [t1, t2, co]\n",
    "                row = pd.Series(r, index=header)\n",
    "                row = pd.DataFrame(r).T\n",
    "                row.to_csv('d3matrix_data.tsv', mode='a', sep='\\t', encoding='utf-8', index=False, header=False)\n",
    "                t2+=1\n",
    "        t1+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
